{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib notebook\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.linalg","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# System Identification\n\nThis notebook will guide you through the process of system identification for linear discrete-time systems by way of numerical examples. Throughout, we will assume that the underlying system dynamics are not changing with time. This is sometimes called \"time-invariance\" in the literature, but that particular phrasing is not important here."},{"metadata":{},"cell_type":"markdown","source":"# 1. Introduction\n\nWe have a discrete-time system with some input $\\vec{u}[i]\\in\\mathcal{R}^{p}$ and a state $\\vec{x}[i]\\in\\mathcal{R}^{n}$. At timestep $i+1$, the value of state $\\vec{x}$ evolves by\n\n$$\\vec{x}[i+1] = A\\vec{x}[i] + B\\vec{u}[i]$$\n\nwhere $A\\in\\mathcal{R}^{n\\times n}$ is the *state transition matrix* and $B\\in\\mathcal{R}^{n\\times p}$ is the *input matrix*. The system $(A, B)$ could represent any kind of physical system whose behavior we want to modify. You've seen how the behavior of an electrical circuit can be described in the form $\\vec{x}[i+1] = A\\vec{x}[i] + B\\vec{u}[i]$ if we choose a time interval $\\Delta$ and sample the appropriate states (voltages on capacitors and currents through inductors). It turns out that a lot of physical systems can be described this way &mdash; electrical systems, mechanical systems, robotic systems, and even biological systems, to name a few. The really great thing about control theory is that it works for all of these.\n\nFor a given physical system, how do we find $A$ and $B$? This is called *finding a model* for the system, or the *modeling problem* in general. If we know a lot about our system, we can sometimes find it directly. For example, if you want to find a model for an electrical circuit, and you know all the component values, you can use circuit analysis to solve for $A$ and $B$. But a lot of the time, we can't do this! The system may too complicated: for example, a hyper-realistic model of car dynamics is critical for controlling an autonomous vehicle, but it would be too difficult to work the model out directly from classical mechanics. Or perhaps the information is hidden: in a biological system, you may not know exactly what biological processes are going on. How do we solve the modeling problem if our system can't be analyzed directly?\n\nWell, if we have access to the system, we can always run some experiments and hypothesize some $A$ and $B$. Suppose we took some sequence of known inputs $\\vec{u}[i]$, fed it into the system, and observed the sequence of $\\vec{x}[i]$ that came out. If we assume that our data follows $\\vec{x}[i+1] = A\\vec{x}[i] + B\\vec{u}[i]$ for some $A$ and $B$, we can use our data to figure out what they are. You could think of this as the scientific method, or as statistics, or as the prototypical kind of machine learning &mdash; but in control theory lingo, it's known as *system identification*. In a way, this is the OG of machine learning. "},{"metadata":{},"cell_type":"markdown","source":"# 2. How to Identify Your System.\n\nPerhaps it makes sense that we can use data gathered from a system to identify an $A$ and $B$ matrix, but how is it actually done?\n\nWell, let's suppose we have data from an experiment that we ran, which started at time $0$ and ended at time $k$. In other words, we have state data\n$$ x[0], x[1],\\dotsc,x[k],$$\nand input data\n$$ u[0], u[1], \\dotsc, u[k-1]. $$\nSuch a data set is called a *trace*.\n\nWe are also hypothesizing, of course, that the state and input data are related by\n$$ x[i+1] = A x[i] + B u[i]. $$\nIf our hypothesis is true, we expect our data to satisfy the equations\n\\begin{align}\nx[1] &= A x[0] + B u[0]\\\\\nx[2] &= A x[1] + B u[1]\\\\\nx[3] &= A x[2] + B u[2]\\\\\n &\\vdots\\\\\nx[\\ell] &= A x[\\ell-1] + B u[\\ell-1].\n\\end{align}\nAs you can see, we have $\\ell$ equations in $n^2+nm$ variables (the variables being the elements of $A$ and $B$). All we have to do is solve this system of equations, and we will have what we wanted! Typically, we will have many more observations than variables (i.e. $k > n^2+nm$), so we will have to solve the system using *least squares*.\n\nThe first step to solving the system is to write it in matrix form. The code we have provided will give you the state and input data in the form of the following matrices:\n\\begin{align}\nU &= \\begin{bmatrix} u[0]^T \\\\ u[1]^T \\\\ \\vdots \\\\ u[\\ell-1]^T \\end{bmatrix}, &\nX &= \\begin{bmatrix} x[0]^T \\\\ u[1]^T \\\\ \\vdots \\\\ x[\\ell]^T \\end{bmatrix}\n\\end{align}\n\nso it would be convenient to write the matrix equation so that the data shows up in roughly that form. Doing that, we get the matrix equation\n$$\n\\begin{bmatrix}\nx[0]^T & u[0]^T\\\\ \nx[1]^T & u[1]^T\\\\ \n\\vdots & \\vdots\\\\\nx[\\ell-1]^T & u[\\ell-1]^T\n\\end{bmatrix}\n\\begin{bmatrix}\nA^T \\\\ B^T\n\\end{bmatrix}\n=\n\\begin{bmatrix} \nx[1]^T\\\\ \nx[2]^T\\\\ \n\\vdots\\\\ \nx[\\ell]^T\n\\end{bmatrix},\n$$\nWhich we can then solve using standard least-squares methods."},{"metadata":{},"cell_type":"markdown","source":"## Example 1: Identifying a System We Already Know\n\nLet's start with a simple example to show you how it works.\nWhat we'll do is create a system whose $A$ and $B$ matrices are known, so that we know if we're right when we identify the system later. To keep things simple, we'll choose a second-order system. It may have come from an RLC circuit, or a robotic system, or something else; for our purposes, it doesn't make any difference. Specifically, we'll use\n\\begin{align}\nA &= \\begin{bmatrix} 0 & 1 \\\\ 0.3 & 0.2 \\end{bmatrix}, & B &= \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}.\n\\end{align}\nLike I said, this is a simple case. We have a system with $n=2$ states and $p=1$ input.\n\nJust out of curiosity, is this system controllable? You can try applying the test you learned about in lecture to see if it is or not.\n\nNow, let's fix these up in NumPy."},{"metadata":{"trusted":false},"cell_type":"code","source":"a_known = np.array([[0,   1],\n                    [0.3, 0.2]])\nb_known = np.array([0, 1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we need to collect some data from this system. We'll use the following function to do this for us. Try following along with the code, and be sure to read the documentation."},{"metadata":{"trusted":false},"cell_type":"code","source":"def make_state_trace(a_matrix, b_matrix, inpt, initial_state=None):\n    \"\"\"\n    make_state_trace: for a given A and B matrices, initial condition, and input, \"run\" the system and \n    calculate the system state for each time step. The length of the trace will be the length of the input plus one.\n    \n    arguments:\n        a_matrix: A matrix of the system (numpy ndarray, n by n)\n        b_matrix: B matrix of the system (numpy ndarray, p by n)\n        inpt: input (u) (numpy ndarray, T by p)\n              initial_state: initial state for the system (numpy ndarray, n by 1). If no initial state is provided, \n              the initial state will be the origin.\n    \n    returns:\n        state_trace: a numpy ndarray containing the state at \n                     each time step that was run (numpy ndarray, T+1 by n).\n    \"\"\"\n    n_states = np.shape(a_matrix)[0]\n    n_timesteps = np.shape(inpt)[0]\n    state_trace = np.zeros((n_timesteps + 1, n_states))\n\n    state_trace[0] = initial_state if initial_state is not None else np.zeros(n_states)\n    for i in range(n_timesteps):\n        state_trace[i + 1] = a_matrix @ state_trace[i] + (b_matrix * inpt[i]).squeeze()\n    return state_trace","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's choose an input to feed into the system, and get a state trace from it.\n\nWhat kind of signal should we use as an input? In principle, it can be anything we want-- but some choices are better than others. One kind of input that generally works quite well is a *random input*, that is where each $u[i]$ is chosen at random. In this notebook, we'll be using random inputs to excite our systems. We can use the following function to generate random inputs. Now, probability itself is a topic that we hold back until CS70. For us here, just think of these as something like tossing coins or rolling dice or spinning a spinner. "},{"metadata":{"trusted":false},"cell_type":"code","source":"def random_input(t, n_inputs, mean=0, std=1):\n    \"\"\"\n    random_input: given a time sequence t, generate a random input trace. Each entry of the trace will be sampled\n                  from a Gaussian distribution with the given mean and standard deviation.\n    \"\"\"\n    random_trace = np.random.normal(loc=mean, scale=std, size=(len(t), n_inputs))\n    return random_trace","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"k = 6\nt = np.arange(k)\n\nu_trace = random_input(t, 1)\n# How did I know to make u1 into a numpy ndarray with dimensions (k,1)? Well, the documentation of make_state_trace \n# says that's how it needs the input trace argument to be.\n\nx_trace = make_state_trace(a_known, b_known, u_trace)\n\n# Let's see what the state trace looks like.\nplt.plot(t, u_trace, label='input')\nplt.plot(t, x_trace[:-1,0], label='state 1')\nplt.plot(t, x_trace[:-1,1], label='state 2')\nplt.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So that's our data set. It's not much, but it's ours. \n\nNow that we have a trace from our known system, let's use least squares to identify $A$ and $B$, and see if we get what we expect.\n\nWe've written the following function to perform least-squares system identification from a trace."},{"metadata":{"trusted":false},"cell_type":"code","source":"def identify_system(state_trace, inpt):\n    \"\"\"\n    identify_system: given an input and the state trace it generated, determine the A and B matrices \n                     of the system that was used to generate the state trace.\n    \n    arguments:\n        state_trace: the state data.\n        inpt: the input data:\n        \n    returns:\n        a_identified: the A matrix identified from the state and input data (numpy ndarray)\n        b_identified: the B matrix identified from the state and input data (numpy ndarray)\n    \"\"\"\n\n    # set up and solve least-squares equation from state and input data\n    n_states = np.shape(state_trace)[1]\n    lsq_b = state_trace[1:, :]\n    lsq_a = np.concatenate((state_trace[:-1], inpt), axis=1)\n    ab_identified, _, _, _ = np.linalg.lstsq(lsq_a, lsq_b, rcond=None)\n    a_identified = ab_identified[:n_states]\n    b_identified = ab_identified[n_states:]\n    \n    return a_identified, b_identified\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's see if the system identification function works with the code you put in. Fingers crossed!"},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"a_identified, b_identified = identify_system(x_trace, u_trace)\nprint(f'Identified A matrix: \\n {np.array2string(a_identified.T, precision=4, suppress_small=True)}')\nprint(f'Identified B matrix: \\n {np.array2string(b_identified.T, precision=4, suppress_small=True)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The previous cell should have printed out an $A$ and $B$ matrix that are very close to the known $A$ and $B$. It's okay if they're a tiny amount off, for example if an element that we know is 0 was identified as 1e-16 or -0 or something &mdash; that's just roundoff error.\n\nIf the true $A$ and $B$ are close to the identified $A$ and $B$, then you have identified your very first system. Good on you!"},{"metadata":{},"cell_type":"markdown","source":"# 3. Variations on System Identification\n\nSo now we know how to use state and input data to identify a model $(A,B)$ of the system that generated that data. This is big step forward. But before it becomes truly practical, we need to examine a few caveats that come up in the real world. What kinds of caveats would those be? Well, here are a few common ones:\n\n  - Our state and input data may be corrupted with some noise.\n  - Instead of directly observing the state, we observe some transformed or rotated state.\n  - Instead of directly observing the state, we view only a single scalar observation at each time step.\n  - your system has too many states to be conveniently worked with, and several seem to be redundant.\n\nIn practice (that is to say, in lab) you will have to deal with some combination of the above problems. In this notebook, we'll show an example of each of these.\n\nThere are a few other caveats to system identification that I think you should be aware of, too. These are:\n  - The behavior of your system is *nonlinear*, meaning it cannot be adequately modeled by an equation of the form $x[i+1]=Ax[i]+Bu[i]$.\n  - Your data has several outliers that cause the least-squares regression to be inaccurate."},{"metadata":{},"cell_type":"markdown","source":"## Generating Random Systems\n\nFor this part of the notebook, we are going to need some new systems to test. In this case, we are going to generate *random* systems, systems whose $A$ and $B$ matrices we don't know yet. The functions provided below will allow us to do just that. Again, be sure to read the documentation. It won't be necessary for you to understand the code (some of it is out of scope), but you might want to check it out anyways. We won't be using all of the functionality of this code in this notebook, but we'll need it later."},{"metadata":{"trusted":false},"cell_type":"code","source":"from itertools import combinations  # Huge shoutout to the combinatorial iterators in itertools, \n                                    # some of Python's lesser-known magic problem solvers.\n\ndef make_random_system(n_states, n_inputs, z_min=0.0, z_max=1.0, must_have_real_eigenvalues=[], \n                       must_have_complex_eigenvalues=[], ccf=False):\n    \"\"\"\n    make_random_system: make a random A and B matrix, whose eigenvalues $z$ lie in the annulus \n    $z_{min}<=|z|<=z_{max}$, occurring as complex conjugate pairs, plus at most one purely real. \n    Additionally, a number of \"must-have\" eigenvalues can be specified that the system is guaranteed to have, \n    which need not lie in the annulus.\n    \n    Specify ccf=True if you want the system to be given in controllable canonical form.\n    \n    arguments:\n        n_states: the number of states you want the sytem to have.\n        n_inputs: the number of inputs you want the system to have.\n        z_min: the smallest allowable magnitude for the random eigenvalues.\n        z_max: the largest allowable magnitude for the random eigenvalues.\n        must_have_real_eigenvalues: a list of purely eigenvalues that the random system is guaranteed to have.\n        must_have_complex_eigenvalues: a list of complex conjugate eigenvalues that the random system is guaranteed to have.\n            You only need to specify one member of the conjugate pair-- the other is implied.\n        ccf: whether or not you want the system matrices to be returned in controllable canonical form.\n    \n    returns:\n        a_matrix: A-matrix of the random system.\n        b_matrix: B-matrix of the random system.\n    \"\"\"\n    n_random = n_states - np.size(must_have_real_eigenvalues) - 2 * np.size(must_have_complex_eigenvalues)\n    assert n_random > 0, 'no random eigenvalues'\n    n_real = n_random % 2\n    n_conj_pairs = int(np.floor(n_random / 2))\n    # This weird probability distribution over the eigenvalue magnitudes ensures that the eigenvalues will be distributed uniformly over the annulus.\n    z_mags = np.sqrt(z_min ** 2 + np.random.rand(n_conj_pairs) * (z_max ** 2 - z_min ** 2))\n    z_angles = 2 * np.pi * np.random.rand(n_conj_pairs)\n    z_random_complex = z_mags * np.exp(1j * z_angles)\n    z_complex = np.concatenate((z_random_complex, must_have_complex_eigenvalues))\n    z_real = must_have_real_eigenvalues\n    if n_real != 0:\n        z_random_real = z_min + np.random.rand(n_real) * (z_max - z_min)\n        z_real = np.concatenate((z_real, z_random_real))\n    if ccf:\n        z = np.concatenate((z_complex, z_real))\n        a_matrix, b_matrix = make_ccf_system_from_eigenvalues(z)\n    else:\n        a_matrix, b_matrix = make_random_system_from_eigenvalues(z_real, z_complex)\n    return a_matrix, b_matrix\n\n\ndef make_random_system_from_eigenvalues(eigvals_real, eigvals_complex, n_inputs=1):\n    \"\"\"\n    Given a sequence of eigenvalues, make a random system at random that has those eigenvalues.\n    \n    arguments:\n        eigvals_real: sequence of real eigenvalues the system is to have.\n        eigvals_complex: sequence of complex eigenvalues the system is to have.\n        \n    returns:\n        a_matrix: A matrix of the system, in CCF\n        b_matrix: B matrix of the system, in CCF (i.e., all zeros except the last element, and the last element is 1)\n    \"\"\"\n    n_states = len(eigvals_real) + 2 * len(eigvals_complex)\n    diag_blocks = []\n    for z in eigvals_real:\n        z_block = np.array([[z]])\n        diag_blocks.append(z_block)\n    for z in eigvals_complex:\n        a, b = np.real(z), np.imag(z)\n        z_block = np.array([[a, b],\n                            [-b, a]])\n        diag_blocks.append(z_block)\n    mcf_mat = scipy.linalg.block_diag(*diag_blocks)  # modal canonical form\n    t_mat = np.random.rand(n_states, n_states) + np.identity(n_states)  # random transformation matrix\n    a_matrix = np.linalg.inv(t_mat) @ mcf_mat @ t_mat\n    b_matrix = np.random.rand(n_states, n_inputs)\n    return a_matrix, b_matrix\n\n\ndef make_ccf_system_from_eigenvalues(eigvals):\n    \"\"\"\n    Given a sequence of eigenvalues, make a system in controllable canonical form that has those eigenvalues.\n    \n    arguments:\n        eigvals: sequence of eigenvalues\n        \n    returns:\n        a_matrix: A matrix of the system, in CCF\n        b_matrix: B matrix of the system, in CCF (i.e. all zeros except the last element, and the last element is 1)\n    \"\"\"\n    n_states = len(eigvals)\n    a_matrix = np.zeros((n_states, n_states))\n    coeffs = roots2coeffs(eigvals)\n    for i in range(n_states - 1):\n        a_matrix[i, i + 1] = 1\n    for i in range(n_states):\n        a_matrix[n_states - 1, n_states - 1 - i] = -np.real(coeffs[i])\n    b_matrix = np.zeros((n_states, 1))\n    b_matrix[n_states - 1, 0] = 1\n    return a_matrix, b_matrix\n\n\ndef roots2coeffs(roots):\n    \"\"\"\n    roots2coeffs: given a list of roots, find the coefficients of the monic polynomial with those roots. \n    The roots can be complex.\n    \n    Specifically, for given roots [r_1, r_2, ..., r_n], we have the monic polynomial\n    \n    p(s) = (s-r_1)*(s-r_2)*...*(s-r_n)\n         = s^n + a_{1}s^{n-1} + a_{2}s^{n-2} + ... + a_{n-1}s + a_n.\n    \n    This function returns the coefficients as list [a_{1}, a_{2}, ... a_{n}].\n    \n    arguments:\n        roots: the polynomial roots (list of complex numbers)\n        \n    returns:\n        coeffs: the coefficients, as described above (list)\n        \n    This function uses Vieta's formulas to calculate the coefficients. For additional info, check out the Wiki page:\n    https://en.wikipedia.org/wiki/Vieta%27s_formulas\n    \"\"\"\n    n = len(roots)\n    coeffs = np.zeros(n, dtype=complex)\n    for i in range(1, n + 1):\n        a = list(combinations(roots, i))\n        prods = np.prod(a, axis=1)\n        s = np.sum(prods)\n        coeffs[i - 1] = np.power(-1, i) * s\n    return coeffs\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In addition to this, it would also be nice to have a function that plots the *eigenvalues* of a given $A$ matrix. That will make it easy to visualize how well the system identification is working. The code below provides such a function. "},{"metadata":{"trusted":false},"cell_type":"code","source":"def plot_eigenvalues(a_matrices, labels=None):\n    \"\"\"\n    plot_poles: given an list A matrix, find their eigenvalues and plot them on the complex plane. \n    Additionally, the unit circle is plotted as a dashed line. Additionally, each A matrix can be given a label, \n    which will be used to identify that matrix's eigenvalues in a legend.\n    \n    arguments:\n        a_matrices: the system A-matrices.\n        labels: list of legend entries, one for each A matrix.\n    \n    returns:\n        None\n    \"\"\"\n    markers=['o','x','1','8','*','+']\n    fig = plt.figure(figsize=(8, 8))    \n    for i, a_matrix in enumerate(a_matrices):\n        eigenvalues = np.linalg.eigvals(a_matrix)\n        if labels:\n            label = labels[i]\n        else:\n            label=''\n        plt.plot(np.real(eigenvalues), np.imag(eigenvalues), linestyle='None', marker=markers[i], ms=5, label=label)\n    t = np.linspace(0, 2 * np.pi, 1000)\n    plt.plot(np.cos(t), np.sin(t), 'k--', label=\"Unit Circle\")\n    plt.xlabel(\"Real Axis\")\n    plt.ylabel(\"Imaginary Axis\")\n    if labels:\n        plt.legend(loc='upper right')\n    plt.grid(True)\n    return None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To try out our new functions, let's generate a random system and plot the eigenvalues of its $A$ matrix. Strictly speaking, there's no limit on how many states (and therefore eigenvalues) our random system can have, but you might want to keep it less than 20 for now."},{"metadata":{"trusted":false},"cell_type":"code","source":"a_rand_1, b_rand_1 = make_random_system(6, 1, z_min=0,z_max=0.2, must_have_real_eigenvalues=[0.3, -0.7])\na_rand_2, b_rand_2 = make_random_system(6, 1, z_min=0,z_max=0.2, must_have_real_eigenvalues=[0.31, -0.71])\nplot_eigenvalues([a_rand_1, a_rand_2], labels=['Random system 1 eigenvalues', 'Random system 2 eigenvalues'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot should show both real systems having the two eigenvalues that we specified ($\\lambda=0.3$ and $\\lambda=-0.7$), and each one having a bunch of other random eigenvalues that all lie within $0 \\le |\\lambda| \\le 0.2$. Try running the above cell with some different parameters, and see what kind of random systems you can make!"},{"metadata":{},"cell_type":"markdown","source":"## Example 2: Identifying a system with using transformed data.\n\nThe first caveat we mentioned is that the data you collect may not represent the true state of the system. Suppose that, instead of having the true state trace\n$$ x[0], x[1],\\dotsc,x[\\ell],$$\nyou instead have a *transformed state trace*, that is\n$$ Tx[0], Tx[1],\\dotsc,Tx[\\ell],$$\nwhere $T\\in\\mathcal{R}^{n\\times n}$ is some full-rank *transformation matrix*. Suppose further that we didn't know that this transformation had occurred-- it could easily happen without our knowledge, or maybe we can't prevent it-- and we ran our system identification function on our transformed state trace and our input trace. What would we get? Well, we certainly wouldn't get the right $A$ and $B$. Instead, we would approximately identify\n$$\n\\begin{align}\nA_{identified} &= TAT^{-1}\\\\\nB_{identified} &= TB.\n\\end{align}\n$$\nAs an exercise, you should prove that this is what we want to happen (i.e. this is the right system matrices to identify if we think of these transformed states as the true states.)\n\nLet's look at a numerical example of this phenomenon. For now, we'll use the same $A$ and $B$ from the first example, but we'll define a transformation matrix\n$$\nT=\\begin{bmatrix}3 & 1 \\\\ 5 & 2 \\end{bmatrix}\n$$\nto apply to the state trace."},{"metadata":{"trusted":false},"cell_type":"code","source":"a_known = np.array([[0, 1],\n                    [0.3, 0.2]])\nb_known = np.array([0, 1])\nt_mat = np.array([[3, 1],\n                  [5, 2]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We've defined the following simple function to apply a transformation to a state trace. This function can also supply a random transformation matrix, but we brought our own this time."},{"metadata":{"trusted":false},"cell_type":"code","source":"def transform_state_trace(state_trace, transformation_matrix=None):\n    \"\"\"\n    transform_state_trace: given a state trace, apply linear transformation to the states.\n    If no transformation is specified, one will be chosen at random.\n    \n    arguments:\n        state_trace: the state trace to be rotated.\n        transformation_matrix: if given, the matrix that specifies the transformation that is to be applied.\n    \n    returns:\n        transformed_state_trace: the state trace, with the transformation applied.\n    \"\"\"\n    n_states = state_trace.shape[1]\n    t_mat = np.random.rand(n_states, n_states) + np.identity(n_states) if transformation_matrix is None else transformation_matrix\n    transformed_state_trace = state_trace @ t_mat.T\n    return transformed_state_trace","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's generate a state trace, apply $T$ to it, and try to identify the system."},{"metadata":{"trusted":false},"cell_type":"code","source":"k = 10\nt = t = np.arange(k)\n\nu_trace = random_input(t, 1)\n\nx_trace = make_state_trace(a_known, b_known, u_trace)\ntx_trace = transform_state_trace(x_trace, t_mat)\n\na_identified, b_identified = identify_system(tx_trace, u_trace)\nprint(\"a_identified:\")\nprint(a_identified)\nprint(\"b_identified:\")\nprint(b_identified)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take a look at the identified $A$ and $B$ matrix. Clearly they're wrong. Are they wrong in the way we expected?\n\nSo, we didn't recover the supposedly underlying $A$ matrix when we use a transformed state trace. But in a way, we're not so far off: even though the identified system is wrong, the identified *eigenvalues* are actually right! We can verify this numerically:"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(np.linalg.eigvals(a_known))\nprint(np.linalg.eigvals(a_identified))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But you should be able to explain, from your knowledge of *coordinate changes*, why this is the case. Actually, a lot more is preserved than just the eigenvalues, but those are clearly one of the most important things that are preserved."},{"metadata":{},"cell_type":"markdown","source":"## Example 3: Identifying a system using noisy data.\n\nSimilar to the above, we also often have nosiy measurements when we are observing some state $x[n]$. We will assume here that the noise is Gaussian-distributed with standard deviation $\\sigma$ and a mean $\\mu$ of zero.\n\nOur noisy observations will be given by\n$$\\begin{align}\nx_{noisy}[i] &= x[i] + w[i] \\\\ u_{noisy}[i] &= u[i] + z[i]\n\\end{align}$$\nwhere $w[i]$ and $z[i]$ are *mean-zero Gaussian noise* with some standard deviation $\\sigma$. In the presense of this noise, can we still identify $A$ and $B$?"},{"metadata":{},"cell_type":"markdown","source":"The following simple function takes a state trace and corrupts it by adding noise."},{"metadata":{"trusted":false},"cell_type":"code","source":"def corrupt_state_trace(state_trace, noise_std=1.0):\n    \"\"\"\n    corrupt_state_trace: corrupt a state trace by addding discrete-time Gaussian white noise to it.\n    \n    arguments:\n        state_trace: the state trace to be corrupted.\n        noise_std: standard deviation of the noise. You can think of this as the \"magnitude\" of the noise. \n                   For example, if noise_std=1, then the noise will usually be between -3 and 3.\n    \n    returns: \n        corrupted_state_trace: the given state trace, plus the noise as specified.\n    \"\"\"\n    noise = np.random.normal(loc=0.0, scale=noise_std, size=np.shape(state_trace))\n    corrupted_state_trace = state_trace + noise\n    return corrupted_state_trace\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, on to the example. This time, we'll generate a random system to use as our \"known\" system."},{"metadata":{"trusted":false},"cell_type":"code","source":"n = 4  # number of state variables in the random system\na_known, b_known = make_random_system(n, 1, z_min=0, z_max=0.6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's generate a state trace, add noise to it, and try to identify the system using the noisy data."},{"metadata":{"trusted":false},"cell_type":"code","source":"k = 1000\nt = np.arange(k)\n\nsigma = 0.01  # how much noise there is\n\nu_trace = random_input(t, 1)\n\nx_trace = make_state_trace(a_known, b_known, u_trace)\nnoisy_x_trace = corrupt_state_trace(x_trace, noise_std=sigma)\n\na_identified, b_identified = identify_system(noisy_x_trace, u_trace)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How did we do? Let's try plotting the eigenvalues to find out."},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"plot_eigenvalues([a_known, a_identified], ['True eigenvalues', 'Identified eigenvalues'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the default values, it looks like the noise didn't affect our ability to identify the system too much. However, if the noise becomes too big, or if we don't take a long enough trace, or if we increase the number of state variables, our identified $A$ and $B$ can potentially be very poor estimates of the true ones."},{"metadata":{},"cell_type":"markdown","source":"## Example 4: Identifying a System Using Only a Scalar Observation.\n\nAnother common case is that, instead of observing the state or even a tranformation of the state at each time step, we instead observe a scalar output state $y[i]=C \\vec{x}[i]$, for some row vector $C\\in\\mathcal{R}^{1\\times n}$. This is sort of like the transformed state data case, except now we have only a very small \"window\" into the system. Essentially, all of our state observations are compressed into a single scalar sequence. Under these conditions, is it even *possible* to identify the system?\n\nYou might be surprised to find that you can actually do quite a lot with scalar observations. We can't exactly proceed as we did before, though. Think about it: since all we have is an input $u[i]$ and a scalar output $y[i]$, we don't even really know how many states our system has! \n\nIt seems like our attempt to construct $A$ and $B$ will be difficult if we don't even know how big they need to be. However, remember that we're just trying to find a model for the system. In this case, there isn't a \"true\" $n$ to use, so we will just have to *choose* an $n$ as part of our design process. If we're lucky, we might guess good value for $n$ &mdash; but then again, we might not. For now, that's the best we can do.\n\nSince we now have a scalar observation trace instead of a full state trace, we have a lot less data to work with. To make things worse, we also have a new unknown part of our model, namely the output matrix $C$. To cope with this, we will assume that $A$ and $C$ take on a special form, which reduces the number of variables we need to solve for. Specifically, we will assume\n\n$$\nA = \\begin{bmatrix}\n0 & 1 & 0 & \\ldots & 0 \\\\ \n0 & 0 & 1 & \\ldots &  0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\ldots & 1 \\\\\n-\\alpha_0 & -\\alpha_1 & -\\alpha_2 & \\ldots & -\\alpha_{n-1} \\end{bmatrix}, \n\\quad C = \\begin{bmatrix}\n1 & 0 & 0 & \\ldots & 0\n\\end{bmatrix}\n$$\n\nThis form of $A$ and $C$ is called *Observable Canonical Form*. In this form, we can write:\n\n$$\ny[i] = x_1[i]\n$$\n\nwhich will allow us to set up a matrix equation that we can solve via least-squares to find the $\\alpha_i$.\n\nThis is a certainly a more convenient form to work with, but is it reasonable to assume that $A$ and $C$ could be in this form? It turns out that, as long as the system is *observable*, we can assume this."},{"metadata":{},"cell_type":"markdown","source":"In this example, instead of using state traces like we have been, we'll be using scalar *output traces*. We'll use the following function to generate output traces from a given system."},{"metadata":{"trusted":false},"cell_type":"code","source":"def make_output_trace(a_matrix, b_matrix, c_matrix, inpt, initial_state=None):\n    \"\"\"\n    make_output_trace: for a given A, B, and C matrices, initial condition, and input, \"run\" the system and calculate the system output for each time step. The length of the trace will be the length of the input.\n    \n    arguments:\n        a_matrix: A matrix of the system (numpy ndarray, n by n)\n        b_matrix: B matrix of the system (numpy ndarray, p by n)\n        c_matrix: C matrix of the system (numpy ndarray, q by n)\n        inpt: input (u) (numpy ndarray, T by p)\n        initial_state: initial state for the system (numpy ndarray, n by 1)\n    \n    returns:\n        state_trace: a numpy ndarray containing the state at each time step that was run (numpy ndarray, T by q).\n    \"\"\"\n    state_trace = make_state_trace(a_matrix, b_matrix, inpt, initial_state)\n    output_trace = state_trace @ c_matrix.T\n    return output_trace","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Due to the special assumption we make about the form of $A$ and $C$, we cannot use the same system identification code from before. To do system identification with outputs, we need a new function that sets up the correct least-squares problem and solves it. The function below does this."},{"metadata":{"trusted":false},"cell_type":"code","source":"def identify_system_from_output(output_trace, inpt, n_states):\n    \"\"\"\n    identify_system_from_output: given an input to a system and the corresponding output, identify the system that \n    was used to transfer the input to the output. The system is identified as an A matrix in Observable Canonical Form,\n    plus a B matrix.\n    \n    arguments:\n        output_trace: the output data.\n        inpt: the input data:\n        \n    returns:\n        a_identified: the A matrix identified from the state and input data.\n        b_identified: the B matrix identified from the state and input data.\n    \n    \"\"\"\n    \n    # set up least-squares equation from state and input data\n    k = np.shape(output_trace)[0]\n    assert k > n_states, 'Not enough data for output system ID with the desired number of states.'\n    n_inputs = np.shape(inpt)[1]\n    \n    lsq_a = np.zeros((k-n_states, n_states + n_states * n_inputs))\n    for j in range(k-n_states):\n        lsq_a[j, :n_states] = output_trace[j:j + n_states].T[0]\n        for w in range(n_inputs):\n            lsq_a[j, n_states + w * n_inputs:n_states + (w + 1) * n_states] = inpt[j:j + n_states, w].T\n        \n    lsq_b = output_trace[n_states:,:]\n    \n    alphab_identified, residuals, rank, s = np.linalg.lstsq(lsq_a, lsq_b, rcond=None)\n    alpha_identified = alphab_identified[:n_states]\n    b_identified = alphab_identified[n_states:].reshape((n_states, n_inputs))\n    a_identified = np.zeros((n_states, n_states))\n    for j in range(n_states - 1):\n        a_identified[j + 1, j] = 1\n        a_identified[j, n_states - 1] = alpha_identified[j]\n    a_identified[n_states - 1, n_states - 1] = alpha_identified[n_states - 1]\n    return a_identified, b_identified","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have those functions, let's try them out. First, we'll need a system."},{"metadata":{"trusted":false},"cell_type":"code","source":"n = 4  # number of state variables in the random system\na_known, b_known = make_random_system(n, 1, z_min=0, z_max=0.6)\nc_known = np.zeros((1, n))\nc_known[-1] = 1  # c_known = [0 0 0 ... 0 1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we'll generate an output trace, do system identification using the input and output data, and then compare the eigenvalues of the true system and the identified system."},{"metadata":{"trusted":false},"cell_type":"code","source":"k = 15\nt = np.arange(k)\nu_trace = random_input(t, 1)\ny_trace = make_output_trace(a_known, b_known, c_known, u_trace)\n\na_identified, b_identified = identify_system_from_output(y_trace, u_trace, n)\n\nplot_eigenvalues([a_known, a_identified], ['True eigenvalues', 'Identified eigenvalues'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, how did it do? Keep in mind that, since we only get a scalar data point at each of the $k$ time steps, we might expect that we need a larger $k$ to get a good estimate for $A$ and $B$ than we did when we had full state data. Try running the above cell with a few different values of $k$, and see how many it takes to get a good estimate. Try combining this with adding noise.\n\nRecall that, in order to be able to estimate $A$ and $B$ from scalar output observations, we said that the system had to be *observable*. If the system isn't observable, it can't be done. Now, consider that you just picked a system totally at random, and tried to identify $A$ and $B$. Given the results you've seen, do you think it's rare or common for a system to be observable?"},{"metadata":{},"cell_type":"markdown","source":"## Example 5: Deliberately Undermodeling a System.\n\nIn the last example, we saw that we don't necessarily need super-detailed information on the system to accurately identify $A$ and $B$. Perhaps we can use that to our advantage.\n\nIf you know that a system has $n$ states, of course the most accurate model of the system will reflect this. However, it may be the case that you have more states than are really necessary to accurately capture the dynamics of your system. In this case, it would be nice if you could discard some states. If you start off with a large amount of states (say $n=50$), it might even be necessary.\n\nHere we will explore what happens when we *undermodel* a system. This is when we choose to model some subset of $q<n$ states from our state trace, in the hopes that we capture the $q$ most important eigenvalues of the true system.\n"},{"metadata":{},"cell_type":"markdown","source":"First, we will create a high-order model, take a state trace, and then take a subset of the states to use for undermodeling."},{"metadata":{"trusted":false},"cell_type":"code","source":"n = 16  # number of state variables in the random system\na_known, b_known = make_random_system(n, 1, z_min=0, z_max=0.05, must_have_real_eigenvalues=[0.9])\n\nk = 100\nt = np.arange(k)\n\nu_trace = random_input(t, 1)\n\nx_trace = make_state_trace(a_known, b_known, u_trace)\n\nq = 3\num_x_trace = x_trace[:, :q]  # take the first q states","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's find a model for the $q$ states we selected for undermodeling."},{"metadata":{"trusted":false},"cell_type":"code","source":"a_um, b_um = identify_system(um_x_trace, u_trace)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Did we capture the major eigenvalues? Let's find out."},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"plot_eigenvalues([a_known, a_um], ['True eigenvalues', 'Undermodeled eigenvalues'])","execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.9.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}